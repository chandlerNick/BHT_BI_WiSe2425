{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storm Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48595, 58)\n",
      "Index(['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH',\n",
      "       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS',\n",
      "       'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME',\n",
      "       'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME',\n",
      "       'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n",
      "       'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE',\n",
      "       'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'TOR_F_SCALE',\n",
      "       'TOR_LENGTH', 'TOR_WIDTH', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE',\n",
      "       'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE',\n",
      "       'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH',\n",
      "       'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON',\n",
      "       'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'LAST_MOD_DATE',\n",
      "       'LAST_MOD_TIME', 'LAST_CERT_DATE', 'LAST_CERT_TIME', 'LAST_MOD',\n",
      "       'LAST_CERT', 'ADDCORR_FLG', 'ADDCORR_DATE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./../Data/Stormdata_2006.csv', encoding='iso-8859-1')\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48595, 58)\n",
      "min: 2006-01-01 00:00:00 \n",
      "max: 2006-10-27 21:00:00\n",
      "(25734, 58)\n",
      "min: 2006-01-01 00:00:00 \n",
      "max: 2006-05-31 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# Convert start and end dates to date time\n",
    "df['END_DATE_TIME'] = pd.to_datetime(df['END_DATE_TIME'])\n",
    "df['BEGIN_DATE_TIME'] = pd.to_datetime(df['BEGIN_DATE_TIME'])\n",
    "\n",
    "print(df.shape)\n",
    "print(\"min:\", min(df['BEGIN_DATE_TIME']), \"\\nmax:\", max(df['END_DATE_TIME']))\n",
    "\n",
    "# Remove those ending after our AOL database - Those before could be ok?\n",
    "df = df[df['END_DATE_TIME'] <= '2006-06-01 00:00:00']\n",
    "\n",
    "print(df.shape)\n",
    "print(\"min:\", min(df['BEGIN_DATE_TIME']), \"\\nmax:\", max(df['END_DATE_TIME']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROLAP Query possibilities:\n",
    "\n",
    "# Count the number of queries on each day, month\n",
    "```\n",
    "SELECT T.BEGIN_MONTH, T.BEGIN_DAY, SUM(*)\n",
    "FROM Storm_Table T, QUERY_TIME_TABLE Q\n",
    "WHERE Q.Day = T.BEGIN_DAY AND Q.Query IS NOT NULL\n",
    "GROUP BY ROLLUP (T.BEGIN_MONTH, T.BEGIN_DAY)\n",
    "```\n",
    "\n",
    "# Count the number of storm events beginning on each day, month\n",
    "```\n",
    "SELECT T.BEGIN_MONTH, T.BEGIN_DAY, SUM(*)\n",
    "FROM Storm_Table T\n",
    "GROUP BY ROLLUP (T.BEGIN_MONTH, T.BEGIN_DAY)\n",
    "```\n",
    "\n",
    "\n",
    "# Obtain the Queries that occur during a natural disaster\n",
    "```\n",
    "SELECT Q.Query, Q.DATE_TIME\n",
    "FROM Storm_Table T, QUERY_TIME_TABLE Q\n",
    "WHERE \n",
    "    Q.Query IS NOT NULL\n",
    "GROUP BY\n",
    "    Q.Query,\n",
    "    Q.DATE_TIME\n",
    "HAVING Q.DATE_TIME BETWEEN T.BEGIN_DATE_TIME AND T.END_DATE_TIME\n",
    "```\n",
    "\n",
    "# Obtain the Queries that occur during a Tornado\n",
    "```\n",
    "SELECT Q.Query, Q.DATE_TIME\n",
    "FROM \n",
    "    QUERY_TIME_TABLE Q,\n",
    "    (\n",
    "        SELECT *\n",
    "        FROM Storm_Table N\n",
    "        WHERE N.EVENT_TYPE = 'Tornado'  -- Can change this line to look at those with damage high cost or injuries too\n",
    "    ) T\n",
    "WHERE \n",
    "    Q.Query IS NOT NULL\n",
    "GROUP BY\n",
    "    Q.Query,\n",
    "    Q.DATE_TIME\n",
    "HAVING Q.DATE_TIME BETWEEN T.BEGIN_DATE_TIME AND T.END_DATE_TIME\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Specific Question:\n",
    "During the event (and 2 weeks after) are people searching things related to the event? (How do we find the keywords associated with this? Any queries containing the 'event type' string?)\n",
    "\n",
    "#### To find the similairty between queries and events:\n",
    "We could try to embed all of the tornado event descriptions and then look at similarity of query embeddings to those (either to the average embedding or pairwise distances), examining the english plaintext of the queries whose embeddings are most similar to the description embeddings.\n",
    "\n",
    "Also, we could look at the urls clicked and see if any of them are related to the weather reporting agency detailed in the source column of the events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH',\n",
      "       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS',\n",
      "       'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME',\n",
      "       'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME',\n",
      "       'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n",
      "       'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE',\n",
      "       'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'CATEGORY', 'TOR_F_SCALE',\n",
      "       'TOR_LENGTH', 'TOR_WIDTH', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE',\n",
      "       'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE',\n",
      "       'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH',\n",
      "       'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON',\n",
      "       'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'LAST_MOD_DATE',\n",
      "       'LAST_MOD_TIME', 'LAST_CERT_DATE', 'LAST_CERT_TIME', 'LAST_MOD',\n",
      "       'LAST_CERT', 'ADDCORR_FLG', 'ADDCORR_DATE'],\n",
      "      dtype='object')\n",
      "(25734, 58)\n",
      "36\n",
      "['Drought' 'WINTER WEATHER' 'Heat' 'Flood' 'High Surf' 'Winter Storm'\n",
      " 'High Wind' 'Wildfire' 'Heavy Snow' 'Ice Storm' 'Debris Flow'\n",
      " 'Heavy Rain' 'Strong Wind' 'Avalanche' 'Hail' 'Tornado'\n",
      " 'Marine Thunderstorm Wind' 'Winter Weather' 'Flash Flood'\n",
      " 'Thunderstorm Wind' 'Lightning' 'Marine High Wind' 'Funnel Cloud'\n",
      " 'Coastal Flood' 'Dense Fog' 'Cold/Wind Chill' 'Frost/Freeze' 'Waterspout'\n",
      " 'Storm Surge/Tide' 'Blizzard' 'Lake-Effect Snow' 'Dust Storm'\n",
      " 'Rip Current' 'Sleet' 'Marine Hail' 'Dust Devil']\n",
      "Tornadoes:  705\n",
      "Wildfires:  191\n"
     ]
    }
   ],
   "source": [
    "# More Exploration\n",
    "\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "print(df['EVENT_TYPE'].nunique())  # 36 unique event types\n",
    "print(df['EVENT_TYPE'].unique())\n",
    "\n",
    "# Most seem to be boring, look at wildfires & tornadoes\n",
    "\n",
    "print(\"Tornadoes: \", len(df[df['EVENT_TYPE'] == 'Tornado']))  # number of tornado events (705)\n",
    "print(\"Wildfires: \", len(df[df['EVENT_TYPE'] == 'Wildfire']))  # number of wildfire events (191)\n",
    "\n",
    "tornado_df = df[df['EVENT_TYPE'] == 'Tornado']\n",
    "wildfire_df = df[df['EVENT_TYPE'] == 'Wildfire']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre shapes:\n",
      "\n",
      "Tornado: (705, 58)\n",
      "Wildfire: (191, 58)\n",
      "Post shapes:\n",
      "\n",
      "Tornado: (705, 44)\n",
      "Wildfire: (191, 29)\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns which contain only NAs for both of the dataframes\n",
    "\n",
    "print(f\"Pre shapes:\\n\\nTornado: {tornado_df.shape}\\nWildfire: {wildfire_df.shape}\\n\")\n",
    "\n",
    "tornado_df = tornado_df.dropna(axis = 1, how = 'all')\n",
    "wildfire_df = wildfire_df.dropna(axis = 1, how = 'all')\n",
    "\n",
    "print(f\"Post shapes:\\n\\nTornado: {tornado_df.shape}\\nWildfire: {wildfire_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH',\n",
      "       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS',\n",
      "       'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME',\n",
      "       'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME',\n",
      "       'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n",
      "       'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE',\n",
      "       'MAGNITUDE', 'MAGNITUDE_TYPE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH',\n",
      "       'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE',\n",
      "       'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT',\n",
      "       'END_LON', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE'],\n",
      "      dtype='object')\n",
      "Index(['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH',\n",
      "       'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE', 'STATE_FIPS',\n",
      "       'YEAR', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME',\n",
      "       'WFO', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME',\n",
      "       'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT',\n",
      "       'DEATHS_INDIRECT', 'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'SOURCE',\n",
      "       'EPISODE_NARRATIVE', 'EVENT_NARRATIVE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the remaining cols\n",
    "print(tornado_df.columns)\n",
    "print(wildfire_df.columns)\n",
    "\n",
    "# Save the dataframes as csvs in the data directory\n",
    "tornado_df.to_csv('./../Data/TornadoEventData.csv', index = False)\n",
    "wildfire_df.to_csv('./../Data/WildfireEventData.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDSP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
